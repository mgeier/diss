\chapter{Spatial Audio From the Beginning}
\label{sec:history}

There were sound waves in the early universe,
as can be observed today via
\emph{baryon acoustic oscillations}%
\footnote{\url{https://en.wikipedia.org/wiki/Baryon_acoustic_oscillations}},
and since that era there have been -- and still are -- many places in the universe
which are filled with an elastic medium that allows sound propagation.
This chapter will focus on sound waves in the atmosphere of planet Earth.
However, any sound travelling
through any medium that inhabits any three-dimensional space
can be considered \emph{spatial sound}.

The terms \emph{spatial sound} and \emph{spatial audio}
can be used interchangeably.
Etymologically, the term \emph{audio},
which is the first-person singular form of the verb \emph{audire}
-- Latin for \emph{hearing/listening} --
implies a sentient being capable of auditory perception
(and, strictly speaking, with Latin language skills)
actually hearing the sound.

Even before life existed on Earth,
its atmosphere (and its oceans as well) must have carried spatial sound,
caused, for example, by storms, meteor strikes or volcanic eruptions
(see figure~\ref{fig:no-observer}).

\begin{figure}[htbp]
\centerline{\tikz \graph {
no initiator [terminal, dotted, font=\itshape] -> [dotted]
natural occurrence -> [dotted]
no observer [terminal, dotted, font=\itshape]
};}
\caption{Spatial sound produced by inanimate processes in a lifeless medium}
\label{fig:no-observer}
\end{figure}

Animals first evolved and diversified in the oceans,
where many of them developed an auditory sense.
When the first animals migrated to land,
they adapted their spatial hearing capabilities
to the new atmospheric medium
or developed entirely new auditory mechanisms.
The development of hearing -- especially spatial hearing --
certainly was (and still is) a very helpful tool for the survival of a species.
It can help predators find prey,
but it can also help prey evade predators
(see figure~\ref{fig:predator-prey}).

\begin{figure}[htbp]
\centerline{\tikz \graph {
"predator" [terminal] -> attack -> potential prey [terminal]
};}
\caption{Spatial sound unintentionally produced by a predator}
\label{fig:predator-prey}
\end{figure}

Apart from sound as an accidental
by-product of locomotion and bodily functions,
animals have developed a wide variety of ways to actively create sounds,
serving a multitude of purposes.
Spatial sound can help finding a mate,
localizing one's own offspring in an overcrowded breeding colony
and in general with intra- and inter-species communication.
In addition to mammals,
several classes of animals are known to be capable of spatial hearing,
like
fish \parencite{popper1993fish},
birds \parencite{macleod2006emu,konishi2003coding}
and even insects \parencite{yager1999structure,schmidt2011cocktail}.
A notable exception is the \emph{praying mantis},
which has an acoustic sense but only a single ear,
making it an \emph{auditory cyclops}
which can most likely not differentiate between
different angles of sound incidence \parencite{yager1986cyclopean}.

Whenever an individual produces sound
and another individual (or even the same one)
perceives it in a way that spatial information is conveyed,
we can consider it a \emph{spatial audio performance}
(see figure~\ref{fig:basic-performance}).

\begin{figure}[htbp]
\centerline{\tikz \graph {
"performer" [terminal] -> performance -> consumer [terminal]
};}
\caption{Spatial audio performance}
\label{fig:basic-performance}
\end{figure}

Like all diagrams in this section, the diagram in
figure~\ref{fig:basic-performance}
is simplified.
There can be multiple performers working together,
but there can also be multiple performances
coalescing into a single experience for a consumer.
There can be multiple consumers, often called an \emph{audience}
(which happens to have the same Latin root as the term \emph{audio}).
There can be feedback from consumers that influences the performance and
consumers may themselves create spatial sound
in form of applause or other audible reactions.
The spatial audio performance
can of course be part of a multi-sensory performance.

Not only the words \emph{performer} and \emph{consumer},
but also the term \emph{performance} itself are used very loosely here.
The performance could simply be the act of communicating
between two animals, including humans.
The auditory and spatial aspect thereof would be covered by the field of
communication acoustics
\parencite{blauert2005communication,pulkki2015communication}.
Instead of (or in addition to) communication,
the goal of the performance could also simply be entertainment.
Whether a performance is artistic or accidental
or anything in between,
here we are interested in the spatial audio aspect of it.

Many animals are capable of spatial audio performances,
but as far as we know,
only humans can write down instructions for others to perform their ideas
(see figure~\ref{fig:performance-with-instructions}).

\begin{figure}[htbp]
\centerline{\tikz \graph {
"creator" [terminal] ->
instructions [document] ->
performance ->
consumer [terminal]
};}
\caption{Spatial audio performance based on written instructions}
\label{fig:performance-with-instructions}
\end{figure}

Of course not every detail of the performance
can be controlled by written instructions,
and performers will have a lot of leeway in interpreting those instructions.
For example, listening to the performance of an ancient Greek tragedy
in an amphitheater
was certainly very much a spatial audio experience,
but the control of spatial audio aspects of the performance
via written stage directions was limited.

Not only theater, but also
traditional musical performances have an inherent spatial component.
An early example for consciously using spatial properties in musical pieces is
the polychoral practice from 16\textsuperscript{th} century Venice,
where multiple choirs are instructed to perform from different places within a
venue, most famously in the \emph{Basilica San Marco di Venezia}.
In modern literature,
this is often referred to as \emph{cori spezzati},
but this term was probably not used at the time
\parencite{bryant1981cori,gembicki2020cori}.

Some classical and romantic operas and symphonies contain short parts
for a separate group of off-stage musicians
-- often positioned outside the main hall --
to achieve an effect of great spatial distance.
Some compositions require
spatially separated groups of musicians
(like
\emph{Notturno in D, K.\,286} by Wolfgang Amadeus Mozart and
\emph{Symphony No.~4} by Charles Ives)
or even multiple full orchestras
(like \emph{Gruppen} by Karlheinz Stockhausen).

The performances described so far were originally intended
to be performed for an audience
located in the same room
as the performers or
-- in case of open air performances --
in the same outdoor area.
With the invention of the \emph{telephone} around 1860,
it became possible to
transmit sound over an electrical wire and listen to it at the far end.
This way, it was possible to listen to an audio performance
without being anywhere near the performers
(see figure~\ref{fig:telephone-transmission}).

\begin{figure}[htbp]
\centerline{
\tikz \graph {
"creator" [terminal] ->
instructions [document] ->
performance ->
"transmission\\via telephone" ->
consumer [terminal]
};}
\caption{Transmitted audio performance}
\label{fig:telephone-transmission}
\end{figure}

The \emph{phonograph} -- invented in 1877 --
allowed to store the recording of an audio performance
(originally in grooves of varying depth on a wax cylinder)
and to reproduce it at a later time
(see figure~\ref{fig:mono-recording}).

\begin{figure}[htbp]
\centerline{\tikz \graph {
"creator" [terminal] ->
instructions [document] ->
performance ->
"phonograph\\recording" [database] ->
reproduction ->
consumer [terminal]
};}
\caption{Recorded audio performance}
\label{fig:mono-recording}
\end{figure}

Even though those inventions were certainly groundbreaking,
the poor sound quality (compared to today's standards) and
the use of only a single audio channel
severely limited the perception of spatial aspects of the performance.
In 1881,
Cl\'ement Ader presented his
\emph{th\'e\^atrophone}
at the \emph{International Exposition of Electricity} in Paris
\parencite{hospitalier1881auditions}.
Instead of a single transmission channel, it used two telephone mouthpieces
mounted on the left and right side of the stage of \emph{Op√©ra Garnier},
connected with two separate telephone lines to two earpieces
(located in a different building in Paris),
which allowed listening to the performances happening on the stage,
including some crude spatial perception of the performers' positions.
The original installation was able to accommodate multiple listeners at once,
each one using a separate pair of mouthpieces,
telephone lines and earpieces.
A
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US257453A}}
was granted in 1882.
In 1890, the system was established as a commercial service
which was in operation until 1932.

The th\'e\^atrophone
had shown the advantages of using two channels instead of one,
but since no amplifiers were available at the time,
applications were limited.
In the following decades,
further development of microphones, amplifiers and loudspeakers
opened up more possibilities.
In 1924, Franklin M. Doolittle was granted a
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1513973A}}
suggesting the transmission of left and right audio signals
over two separate AM radio channels.
Doolittle also made experimental broadcasts from his
radio station WPAJ in New Haven, Connecticut
\parencite{doolittle1925binaural}.
In his studio,
two microphones with a center-to-center distance of
about \qty{18}{\centi\meter} were used to pick up sound.
The two signals were broadcast over two separate radio frequencies
and listeners had to use separate AM receivers for each frequency.
The distance between microphones was based on the distance between human ears
and broadcasts were mainly intended for headphone reproduction.
In a
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1624486A}}
from the year 1927,
Harvey Fletcher and Leon Sivian
suggest the use of an artificial head containing two microphones near the ears
to simulate the acoustic scattering of a real human head which
strongly affects spatial perception.
\textcite{fletcher1933illusion}
describes a realization of this idea in the form of
an acoustic manikin named \emph{Oscar}.

\begin{figure}[htbp]
\centerline{\tikz \graph {
"creator" [terminal] ->
instructions [document] ->
performance ->
"two-channel\\recording" [database] ->
"binaural\\reproduction" ->
consumer [terminal]
};}
\caption{Binaural recording of a spatial audio performance}
\label{fig:binaural-recording}
\end{figure}

Just as sound \emph{transmission} was extended to more than one channel,
sound \emph{recording} followed suit
(see figure~\ref{fig:binaural-recording}).
Another
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1817177A}}
by Doolittle
-- published in 1931 but filed already in 1921 --
describes how two channels can be stored on a phonograph record
with two grooves running side by side.
The tonearm would have two needles next to each other
to reproduce the left and right signals.
A very similar approach is described in a 1924
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1508432A}}
by
Harry Wier (also filed in 1921).
A 1932
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1855149A}}
by W.\ Bartlett Jones (filed in 1927)
mentions multiple variations of two-channel phonographs,
including one using a record with
a single groove with variations in both depth and lateral shift.
This was later known as \emph{V/L} for \emph{vertical/lateral}.
Alan Dower Blumlein's UK
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=GB394325A}}
from 1933 (filed in 1931)
suggests rotating the single groove recording apparatus by 45 degrees,
so that the variations of the two channels
still happen at an angle of 90 degrees to each other,
but at 45 degrees relative to the disk surface.
This type of record is also known as \emph{45/45}.
A few years later,
the same \emph{45/45} approach was also proposed in a US
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US2114471A}}
submitted by Arthur Keller and Irad Rafuse.
The \emph{45/45} method is still used in today's vinyl records,
which are once again quite popular,
despite the abundance of modern digital storage media.

Most of the reproduction systems mentioned so far were mainly targeted for
headphone listening.
The goal was to
place two microphones at the same distance as between two ears
to create appropriate phase differences.
Ideally, some kind of dummy head was used
to model the acoustic shadow of a real head.
Since the two signals are meant to be reproduced very close to
the two ears of a listener, respectively,
this method is called \emph{binaural} reproduction,
after the Latin words \emph{bis} and \emph{auris},
which mean \emph{two times} and \emph{ear}.
An overview of the history of binaural recording technology can be found in
\parencite{paul2009binaural}.

The idea of using more than one channel was of course also applied to
loudspeaker-based reproduction.
This was especially relevant for movie theaters.
Even when commercial movies were still silent
(the first feature film with sound
-- \emph{The Jazz Singer} --
came out in the year 1927),
inventors were already thinking about multi-channel sound for movies.
A
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1124580A}}
by Edward Amet (filed 1911, granted 1915)
describes a device that uses a mono phonograph recording
which is switched (not panned!) between multiple telephone receivers.
Loudspeaker technology was still in its infancy,
and apparently telephone receivers were state of the art
for sound reproduction.
The telephone receivers would be placed at different positions
close to the screen,
and the idea was that switching between them allowed the sound
to follow the corresponding actions shown on the screen,
or as it is phrased in the patent text,
``By the means shown a picture of a moving sound-making object
may be accompanied in its travel across the screen by its
appropriate reproduced sound.''
The switching of the signal was meant to be achieved by means of
electrically conducting lines mounted on small insulating plates
-- individually hand-crafted for each phonograph record used --
touching an electric contact point
which moves together with the tonearm of the phonograph.

In a
patent\footnote{\url{https://worldwide.espacenet.com/patent/search?q=US1589139A}}
from 1926,
Earl H.\ Foley
suggests using
a three-channel recording
reproduced over loudspeakers at the left and the right and behind the
center of the movie screen.
The sound track is to be recorded with three microphones
placed across the field of view when shooting the movie.
The goal is again for the sound to follow the movements
of the performers visible on screen.

Three channels were also used in 1933, when a performance of the
Philadelphia Symphony Orchestra
was picked up in the \emph{Academy of Music} in Philadelphia
with three microphones,
transmitted via three telephone lines
and reproduced in \emph{Constitution Hall} in Washington, D.\ C.,
with three loudspeakers.
The system and its capability for spatial audio reproduction
has been presented in a
\emph{Symposium on
Wire Transmission of
Symphonic Music and its
Reproduction in Auditory Perspective}.
The microphone and loudspeaker setup that was used in the two halls is shown in
\parencite{bedell1934auditory}.
\textcite{steinberg1934auditory} describe
additional speech localization experiments that have been conducted
with three loudspeakers
in the auditorium at the \emph{Bell Telephone Laboratories}
connected to three microphones in a smaller pick-up room.
Using only two channels instead of three still provided
good angular localization from a center listening position, albeit
with slightly less accurate depth perception.
As the observer moved to one side, however,
the virtual source shifted more rapidly toward the nearer loudspeaker
than in the three-channel setup.
They conclude that ``2-channel
reproduction of orchestral music gives good satisfaction, and the
difference between it and 3-channel reproduction for music probably
is less than for speech reproduction or the reproduction of sounds
from moving sources.''

A three-channel sound track was also used
in the first commercial feature-length movie with stereophonic sound
-- Walt Disney's \emph{Fantasia} -- which premiered in the year 1940.
The movie consists of
a series of classical music pieces, accompanied by animation sequences in
different styles.
One of the segments
features none other than Mickey Mouse
as the titular character in Paul Dukas's
\emph{Sourcerer's Apprentice}.
A narrator gives some explanations
in-between the segments,
and Mickey Mouse has a short conversation with Leopold Stokowski --
the conductor of the sound track --
but there is no spoken word during the music pieces
and there are no sound effects.
This means that any spatial audio positioning and movement
was only applied to the music,
which is not very common nowadays.

Most of the sound track was recorded in the
\emph{Academy of Music} in Philadelphia
onto
eight tracks of optical film.
Six tracks were used for recording different sections of the orchestra,
one track held a mono mix of those first six tracks
and one track contained a distant pick-up of the whole orchestra.
These tracks were afterwards mixed in a so-called
\emph{re-recording} process, resulting in three program audio channels
destined for the left, center and right loudspeakers, respectively.
To be able to dynamically distribute a single recorded track
between the three loudspeakers,
while at the same time keeping the total power constant,
a device nicknamed \emph{``The Panpot''} was used.
This name is still used today
and the whole process is nowadays called \emph{panning}.

Optical sound tracks at the time did not have enough dynamic range
to faithfully reproduce the sound of an orchestra.
To overcome this, three variable-gain amplifiers were used,
which were controlled by the amplitudes of three sine tones
at constant frequencies of \qty{250}{}, \qty{630}{} and \qty{1600}{\hertz},
respectively.
These control tones were generated in the aforementioned re-recording process
and were recorded onto a separate channel besides the three program channels.
The four channels were then printed on 35-mm film.
When \emph{Fantasia} was shown in movie theaters,
this film was played back in synchrony with a second film
containing the moving images.
The channel with the three control tones was fed into the
Tone-Operated Gain-Adjusting Device (TOGAD),
which in turn controlled the gain stages of the three amplifiers.
Those amplifiers were of course using vacuum tubes,
which was state of the art at the time.
The whole technology was called \emph{Fantasound}
\parencite{garity1941fantasound}.

Modern texts sometimes claim that
the TOGAD has also been used
for panning and even for moving sounds around the audience,
and some texts mention a relatively large number of loudspeaker channels.
With only three control tones and three input channels,
the panning options would have been very limited, though.
Therefore, we can infer that the control tones
were mostly used for increasing the dynamic range.
However,
in addition to the three main loudspeakers behind the projection screen,
there were indeed auxiliary loudspeakers placed in the auditorium.
But those were manually switched on by the projectionist
in the last segment of the movie -- \emph{Ave Maria} --
and they were fed by two of the main three sound channels.
In some of the installations,
the switching was automated by a mechanical relay system
operated by means of notches on the edge of the film.

The \emph{Fantasound} system
required a lot of equipment that was generally not available at cinemas.
Therefore, the movie was presented as a road-show only at selected cinemas,
where the custom equipment was installed temporarily.
Partly because of the large cost of these installations,
\emph{Fantasia} was not a big commercial success.
Another reason for discontinuing the project was the involvement of the US
in the Second World War.

At the time,
the terms \emph{stereophonic} and \emph{binaural}
were often used interchangeably.
However, \textcite{snow1953stereo} makes a clear distinction
between the terms as they are still used today:
\emph{binaural} recordings are made with two microphones
-- preferably in an artificial head --
and intended for headphone reproduction,
while \emph{stereophonic} recordings are made with two or more microphones
and intended for reproduction with two or more loudspeakers
``spaced in front of a listening area.''
It is important to note that even though two-channel recordings
were -- and still are -- very common,
stereophony is not limited to two channels at all.
The Greek word \emph{stereos} simply means \emph{firm} or \emph{solid}.
The term \emph{stereophonic} was most likely inspired
by \emph{stereoscopic} images and photographs \footnote{\url{%
https://en.wikipedia.org/wiki/Stereoscopy}},
which were all the rage
in the second half of the 19\textsuperscript{th} century.

After the Second World War,
magnetic tape was rapidly displacing
phonograph records and optical film as a recording medium and
in the 1950s,
tape recorders were also conquering the concert halls.
Electronic sounds and natural sounds were recorded on tape,
edited and often electronically modified.
Those tapes were then played back during a concert in a concert hall.
For example, the composition \emph{Williams Mix} by
John Cage, composed between 1951 and 1953,
was realized on eight heavily spliced mono tapes,
which were played back by eight separate tape machines
with eight equidistant loudspeakers around the audience.

Olivier Messiaen's composition \emph{Timbres-dur√©es}
was first performed in 1952
from a four-track tape using four loudspeakers.
Two tracks were assigned to the left and right loudspeakers
in front of the audience.
Another track was played back over a loudspeaker behind the audience
and one mounted on the ceiling above the audience.
Finally, one special track called \emph{cin√©matique}
was interactively distributed over all four loudspeakers
using a
spatialization device
called
\emph{pupitre d'espace},
developed Jacques Poullin.
This device consisted of four rings of \qty{50}{\centi\meter} diameter
placed around the conductor,
representing the four loudspeaker positions left, right, above and towards the
rear end of the room.
The conductor was holding an electrical coil in his hand
and moving this coil with big gestures between the rings
made the sound move between the four loudspeakers
\parencite{battier2015discoveries}.

Another famous example is
the piece \emph{Kontakte} by Karlheinz Stockhausen
(composed 1958--60),
in which he used a rotation table on which a loudspeaker was mounted on and
which was manually rotated.
The resulting sound was recorded on
tape via four microphones at fixed positions around the table.
During a concert, the four-track tape was played back on four loudspeakers
around the audience.

The concept of playing tapes over loudspeakers
was taken to the next level
at the Philips Pavilion on the 1958 Brussels World's Fair.
Edgar Var√®se composed a spatial music piece as part of
an eight-minutes-long multimedia spectacle
called \emph{Po√®me √©lectronique},
involving a wardrobe-sized 3-track magnetic tape machine,
20 amplifiers and 350 loudspeakers
\parencite{kalff1958poem}.
The routing of the audio tracks to groups of loudspeakers,
as well as the control of illumination effects,
was facilitated by another magnetic tape machine,
running in synchrony with the first.
The second tape contained 15 tracks
with 12 fixed-frequency control signals per track.
Some of the continuous signals controlled relays to activate
certain loudspeaker groups,
others were used to select which of the three audio tracks to use
as input for those groups.
A part of the loudspeakers formed five so-called \emph{sound routes}
along the inside walls of the pavilion,
which created the illusion of moving sound sources.
A train of pulses in one of the control signals
was used to turn a rotary control to switch from one loudspeaker to the next.
The rate of these pulses -- up to 10 per second --
determined the speed of the apparent sound source.
In the following years,
many more spatial audio compositions for tape and loudspeakers were created,
for example
\emph{Bohor} (1962) by Iannis Xenakis
and
\emph{HPSCHD} (1969) by John Cage.

Visitors of the 1970 World's Fair in Osaka, Japan,
had the opportunity to experience
large-scale spatial music performances
in more than one custom-built pavilion on the same exhibition grounds.
The German Pavilion was a spherical building housing a spherical auditorium.
50 groups of loudspeakers were mounted at different heights,
surrounding a central listening platform from all directions,
including above and below.
Compositions by Stockhausen for
live soloists and multichannel tape
were performed with live spatialization.
During the performances,
the tracks could either be directly routed to fixed loudspeaker positions
or via two custom-built
\emph{rotation mills}
which could be used to move sounds
along trajectories of 10 loudspeaker groups each
by turning a hand crank
\parencite{bates2015kontakte}.

Another notable venue at the Osaka Expo in 1970 was the
\emph{Space Theatre}
in the
so-called \emph{Steel Pavilion} of the \emph{Japan Iron and Steel Federation},
where
Iannis Xenakis' composition \emph{Hibiki-Hana-Ma} was performed.
A 12-track tape was spatialized along elaborate trajectories
according to control commands
that were recorded on film.
The exact number of loudspeakers is unclear,
but maybe 264 groups of loudspeakers were installed in the pavilion,
with a total of 800 individual loudspeakers
\parencite{paland2015xenakis}.

While most of the compositions mentioned above were made
without the help of computers (Cage's \emph{HPSCHD} being a notable exception),
the use of computers became much more common in the 1970s.
The computer-generated signals, however,
were still often recorded on analog tapes to be used for the performances.
In 1972,
John Chowning composed the piece \emph{Turenas},
which was to be played on
four loudspeakers placed in the corners of a square.
Both the sound synthesis
-- using the recently discovered FM synthesis --
and the spatialization
-- using elaborate trajectories based on Lissajous curves --
were realized with the same computer program.
It was created on a DEC PDP-10 mainframe
using the \emph{Music 10} programming environment,
which itself was written in a mix of assembly language and Fortran
\parencite{chowning2011turenas}.
The method used for spatializing moving sound sources is described in
\parencite{chowning1971moving}.


\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    \matrix [row sep=1em, column sep=1em, name=matrix] {
        \node (pi) [document] {performance\\instructions}; &
        \node (p) [process] {performance}; \\[-1.5ex]
        & \node (rec) [database] {audio\\tracks}; \\
        \node (mi) [document] {mixing\\instructions}; &
        \node (m) [process] {playback \&\\live mixing}; \\
    };
\node (creator) [terminal, left=of matrix] {creator};
\node (consumer) [terminal, right=of m] {consumer};
\graph [use existing nodes] {
    creator -> [bend left=25] pi -> p -> rec -> m;
    creator -> [bend right=25] mi -> m -> consumer;
};
\end{tikzpicture}
\caption{Tape-based multi-channel audio performance}
\label{fig:tape-mixing}
\end{figure}

The principle of playing back pre-recorded audio tracks
and distributing them among loudspeakers during a concert
is shown in figure~\ref{fig:tape-mixing}.
In some instances, composers produce a score
which is used by other people to create appropriate tapes for performances.
Often, however, the composers would record and edit the tapes themselves.
Note that typically,
the composer gives instructions for
the recording and editing of the tracks beforehand
as well as for their spatialization during the concert
(which may be automated or done manually).
This differs from classical orchestra recordings
(see figure~\ref{fig:multi-mic}),
where the composer only gives instructions for the performance itself.

After spatial audio being successfully used in concert halls and cinemas,
it was just a matter of time that it would also find its way into private homes.
Commercial two-channel stereo records and the corresponding reproducers
were widely available to consumers since the late 1950s.
Later, reproduction systems with four loudspeakers --
to be placed in the corners of a square,
surrounding the listener from all directions in the horizontal plane --
were sold under the name \emph{Quadraphony}
\parencite{woodward1977quadraphony}.
In 1967, \emph{Pink Floyd} gave a live concert
using a quadraphonic loudspeaker setup
controlled by the \emph{Azimuth Co-ordinator}\footnote{%
\url{https://collections.vam.ac.uk/item/O76817/}}
panning device.
Their album \emph{The Dark Side of the Moon}
was released a few years later in quadraphonic sound.
Even though a number of quadraphonic records were produced and sold in the 1970s,
the technology never really took off on the mass market.

\textcite{cooper1970tetrahedral}
suggested to extend the horizontal quadraphonic system
with a height dimension
by using a tetrahedral microphone and loudspeaker setup.
The suggested setup was quickly superseded by a more systematic approach by
\textcite{gerzon1970quadraphonic_part_two}.
This approach --
which was based on \emph{spherical harmonics} --
was generalized to larger numbers of loudspeakers
\parencite{gerzon1973periphony}
and later became known as \emph{Ambisonics}
\parencite{fellgett1975ambisonics1,gerzon1975ambisonics2}.

The quadraphonic loudspeaker setup
-- with a loudspeaker in each corner of a square --
was not much used beyond the 1970s.
This was probably because the movie industry did not want to give up
the center loudspeaker behind the projection screen,
which was especially useful for a stable perception of the dialogue
in the whole auditorium.
Cinemas were using a different setup for four-channel sound tracks:
they used the traditional left\slash center\slash right positions behind the screen,
while the fourth channel was providing ambient sounds from the back wall.
Later, a fifth channel was added in order to be able to position
ambient sounds between the left and the right side behind the audience.
An additional, bandwith-restricted channel was used to provide
low frequency effects.
Variations of this setup were used throughout the 1980s.
In the early 1990s the loudspeaker layout was standardized by
ITU-R as Recommendation BS.775\footnote{%
\url{https://www.itu.int/rec/R-REC-BS.775/}}
-- also known as \emph{5.1 surround} -- which is still widely used today.
In the following years,
more setups with more and more loudspeakers
-- including loudspeakers at different heights --
have been proposed,
up to \emph{22.2 surround}
\parencite{hamasaki2004-22.2,hamasaki2005-22.2}.

The signals for the different loudspeaker channels are typically obtained
by using a combination of microphone techniques and panning.
As an example,
figure~\ref{fig:multi-mic} shows a typical modern procedure for recording
an orchestral performance.

\begin{figure}[htbp]
\centerline{%
\begin{tikzpicture}[start chain, every node/.append style=join, every join/.style={->}]
    \node [on chain, terminal] {creator};
    \node [on chain, document] {instructions};
    \node [on chain, process] (p) {performance};
    \node [on chain=going below, process, node distance=3em, multiple] (spot) {spot\\microphones};
    \node [on chain=going below, process, node distance=3em] (m) { mixing/panning };
    \node [on chain=going below, database] {stereophonic\\recording};
    \node [on chain, process] {stereophonic\\reproduction};
    \node [on chain, terminal] {consumer};

    \node [process, left=of spot] (main) {main stereo\\microphone};
    \node [process, right=of spot, multiple] (room) {room\\microphones};

    \graph [use existing nodes] {
        p -> [out=-120, in=north] main -> [out=south, in=120] m;
        p -> [out=-60, in=north] room -> [out=south, in=60] m;
    };
\end{tikzpicture}%
}
\caption{Modern multi-microphone orchestra recording}
\label{fig:multi-mic}
\end{figure}

Pairwise panning has been used for many decades
with a multitude of panning devices and panning curves.
Modern panning techniques
which allow for arbitrary three-dimensional loudspeaker layouts
include
Vector Base Amplitude Panning (VBAP)
\parencite{pulkki1997vbap},
Distance-Based Amplitude Panning (DBAP)
\parencite{lossius2009dbap}
and All-Round Ambisonic Panning (AllRAP)
\parencite{zotter2012allrap}.

Our ability to localize sounds in stereophonic reproduction is
based on a psychoacoustic effect called
\emph{summing localization}
\parencite{theile1980lokalisation}.
This is known to work well
if the listener position is at equal distance to each loudspeaker
-- in a point that's called \emph{sweet spot}.
However, it does not work very well at all between
the left back and left front loudspeaker
nor between the right front and right back loudspeaker
\parencite{theile1977localization}.

To overcome these limitations, an alternative approach
-- aiming at physically accurate reconstruction of a sound field
and therefore termed \emph{sound field synthesis} --
has been brought forward.
It is based on Huygens' principle\footnote{\url{%
https://en.wikipedia.org/wiki/Huygens‚ÄìFresnel_principle}},
which has
already been discovered in the 17\textsuperscript{th} century.
The idea was maybe first mentioned by \textcite{fletcher1934basic}
who describes a hall with an acoustically transparent curtain
with microphones ``scattered uniformly over it''
mounted between the orchestra and the audience.
The signals picked up by those (hypothetical) microphones could then
be transmitted to a different hall with a similar curtain,
but with loudspeakers instead of microphones affixed to it.
In this thought experiment,
the audience in the second hall
``should obtain the same effect
as those listening to the original music''
if the two halls have the same
size, shape and acoustical properties and
-- last but not least --
the number of microphones and loudspeakers is infinite
and they are infinitesimally small.
This fictional setup is brought up again by
\textcite{snow1953stereo},
who, instead of using the word \emph{curtain},
mentions a \emph{screen} consisting of an extremely large number of
extremely small microphones
and a corresponding \emph{screen} of extremely small loudspeakers.
Talking about reality again,
he makes sure to clarify that, when using a practical setup
of two or three loudspeakers,
a different hearing mechanism
is used by the brain.
Decades later,
the principle was mathematically formalized using
Rayleigh's first integral equation
\parencite{berkhout1988holographic,berkhout1993acoustic}.
This finally led
-- together with advances in amplifier, loudspeaker
and digital signal processing technology --
to an actually realizable system
named Wave Field Synthesis (WFS).
Still in theory,
the \emph{screen} of loudspeakers would completely enclose
a volume around the listener,
but in order to be built in practice,
a single horizontal line
of small loudspeakers would be mounted at ear height
around the listening area.
Originally,
only linear loudspeaker arrays were supported,
but the method has been extended to allow for
arbitrary convex
loudspeaker layouts
-- but with all loudspeakers still located in a horizontal plane
\parencite{spors2008wfs}.

Because of the large number of loudspeakers,
it was not practical to record each loudspeaker signal on its own channel
for later reproduction.
This would be called \emph{channel-based} reproduction,
which is the typical way to store recordings for stereophonic systems
like 5.1, for example.
Not only do stereophonic systems have a smaller number of loudspeaker channels,
but they also have standardized loudspeaker layouts,
which makes it possible to reproduce the same \emph{channel-based} recording
on any compatible system.
In contrast, WFS systems typically not only have many more loudspeakers
than stereophonic systems, but they also have very different
-- and nearly arbitrary --
loudspeaker layouts,
which would mean that a recording for one system
would not be able to be played back on a different WFS system.
This led to a new paradigm for the storage of recordings called
\emph{object-based} reproduction
\parencite{pereira2002mpeg4,geier2010future,geier2010object-based,tsingos2018object-based}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    \matrix [row sep=1em, column sep=1em, name=matrix] {
        \node (pi) [document] {performance\\instructions}; &
        \node (p) [process] {performance}; \\[-1.5ex]
        & \node (rec) [database] {source\\signals}; \\
        \node (mi) [document] {scene\\description}; &
        \node (m) [process] {rendering}; \\
    };
\node (creator) [terminal, left=of matrix] {creator};
\node (consumer) [terminal, right=of m] {consumer};
\graph [use existing nodes] {
    creator -> [bend left=25] pi -> p -> rec -> m;
    creator -> [bend right=25] mi -> m -> consumer;
};
\end{tikzpicture}
\caption{Object-based audio reproduction}
\label{fig:object-based}
\end{figure}

The \emph{object-based} production workflow is depicted schematically
in figure~\ref{fig:object-based}.
Instead of storing the signals of all loudspeakers,
the source signals -- or sub-mixes -- are stored
together with some data indicating when and from which spatial positions
these signals should be heard by the audience.
These source signals together with their associated spatio-temporal
and other information
are called \emph{scene description}.
From this scene description,
the loudspeaker signals are then generated in real time
during reproduction in a procedure called \emph{rendering}.
Since the scene description ideally doesn't contain any information about the
reproduction system,
it can be rendered on any system,
regardless of number and layout of loudspeakers.
Existing \emph{channel-based} recordings can still be used
in \emph{object-based} systems by placing the individual channels
at appropriate positions in the scene description.
\Textcite{theile2003potential} call this
\emph{virtual panning spots}.
Note that figure~\ref{fig:object-based} has the exact same structure as
figure~\ref{fig:tape-mixing}.
In a way, \emph{object-based} reproduction can be viewed
as a modern reincarnation
of the tape-based spatial music techniques from the 1950s and 1960s.

In the 1990s,
another method for \emph{sound field synthesis} was developed
by extending the above-mentioned \emph{Ambisonics} approach.
Originally, spherical harmonics of zeroth and first order
had been used to create driving signals for four loudspeakers
mounted in a three-dimensional tetrahedral arrangement.
This led to a rather low spatial resolution,
but by using higher orders
-- and a correspondingly larger number of loudspeakers --
the accuracy of the reproduced sound field could be improved.
To distinguish it from the original approach,
this was given the name Higher-Order Ambisonics (HOA).
The original theory of Ambisonics assumed that
the distances to the loudspeakers
are very large, leading to incoming plane wave fronts
within the designated listening area.
However, the distances in real loudspeaker setups are much smaller,
which causes curved wave fronts and therefore errors in the reproduced
sound field.
To overcome this,
Near-Field-Corrected HOA (NFC-HOA) has been developed
\parencite{daniel2000representation,daniel2003spatial}.

For a three-dimensional HOA system of order $N$,
the number of spherical harmonics components
-- \ie the number of storage channels --
is $(N+1)^2$.
For example, a third order Ambisonics system needs \qty{16}{} storage channels.
For two-dimensional horizontal systems,
only $2N+1$ circular harmonics components
-- and therefore storage channels -- are needed.
These component signals
-- also called \emph{B-format} --
are independent of the targeted loudspeaker positions.
Given a certain loudspeaker setup,
the Ambisonics components have to be \emph{decoded} at the reproduction site
in order to generate the appropriate loudspeaker driving signals.

Since the loudspeaker signals are created on the fly
and not used for storage or transmission,
HOA is not considered a \emph{channel-based} method.
However, there seems to be no consensus on how to call it instead.
\textcite{spors2013review} use the term
\emph{transform domain-based},
\textcite{robinson2015cinematic,nicol2018soundfield} use the term
\emph{sound field-based}
and others simply call it \emph{HOA-based}
for the lack of a better term.
There are even authors who for no apparent reason call it \emph{scene-based},
which is quite misleading since the word \emph{scene}
is already being heavily used in the context of
\emph{object-based} reproduction
and the term \emph{scene-based} has already been used earlier
for something completely different by \textcite{rumsey2002spatial}.

Storing and/or transmitting Ambisonics component channels
is not the only way to use HOA, though.
It can also be used on the reproduction side of an \emph{object-based} system
by means of real-time Ambisonics amplitude panning
\parencite{neukom2008ambisonics,zotter2019ambisonics}.
This way, high Ambisonics orders can be used
without having the need for storing the Ambisonics component signals.
However, this is not advantageous if the number of simultaneous source signals
is larger than the number of Ambisonics components for the desired order.

For more information about loudspeaker-based reproduction see
\parencite{blauert2012synopsis,spors2013review}
and for the mathematical fundamentals of sound field synthesis
see \parencite{ahrens2012analytic}.

As mentioned earlier,
the history of spatial audio transmission and recording
started towards the end of the 19\textsuperscript{th} century
with \emph{binaural} transmission and reproduction.
The usage of an artificial head
with microphones mounted in it
has already been described in the late 1920s.
Stereophonic loudspeaker
reproduction came later, but it slowly took over nearly all of the market.
Binaural reproduction had a very short-lived renaissance in the 1970s,
where several dummy head recordings were produced
and gained some popularity, but were later quickly forgotten again
by the general public.

Another milestone in binaural technology
was the measurement of Head-Related Transfer Functions (HRTFs),
which compactly represent the acoustic effect of a listener's
outer ears, head and torso on an incoming sound,
depending on the direction of incidence.
This enabled \emph{binaural rendering} of object-based audio scenes.
By using a \emph{head tracker},
the rotation of the listener's head
can be compensated for in real time
\parencite{wenzel1990convolvotron}.
This process is known as
\emph{dynamic binaural rendering} or
\emph{dynamic binaural synthesis}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[start chain, every join/.style={->}]
    \node [on chain, process, join] {performance};
    \node [on chain=going below, database, join] (multi) {multi-channel\\recording};
    \matrix [below=of multi, row sep=1em, column sep=1em, name=matrix] {
        \node (a0) [process] {mixing}; &
        \node (a1) [process] {mixing}; &
        \node (a2) [process] {mixing}; \\
        \node (b0) [database, minimum width=4em, minimum height=3.5em] {2.0\\mix}; &
        \node (b1) [database, minimum width=4em, minimum height=3.5em] {5.1\\mix}; &
        \node (b2) [database, minimum width=4em, minimum height=3.5em] {22.2\\mix}; \\
    };
\graph [use existing nodes] {
    multi -> {a0, a1, a2} -> {b0, b1, b2}
};
\end{tikzpicture}
\caption{Multiple mixes for different output formats}
\label{fig:multiple-mixes}
\end{figure}

Figure~\ref{fig:multiple-mixes}
shows a potential problem with \emph{channel-based} techniques:
since multiple different standardized loudspeaker setups are in general use,
each production has to be manually mixed multiple times
-- once for each target setup.
Automatic up- and down-mixing methods exist,
but they often lead to reduced sound quality
\parencite{zielinski2003effects,vilkamo2014reduction,avendano2004frequency-domain,faller2006multiple-loudspeaker}.
As an alternative,
the \emph{object-based} approach can be used for
\emph{system-independent mixing}.
A scene description is created only once,
and arbitrary \emph{channel-based} mixes
can be rendered automatically.
This approach also allows \emph{binaural monitoring} of loudspeaker systems
\parencite{geier2009binaural}.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    \matrix [row sep=1em, column sep=2em, name=matrix] {
        \node (or1) [process,multiple] {original\\recordings}; &
        \node (mp) [process] {mixing\\\footnotesize(with panning)}; &
        \node (rec) [database] {multi-channel\\recording}; &
        \node (rep) [process] {stereophonic\\reproduction}; &
        \node (cons1) [terminal] {consumer}; \\
        \node (or2) [process,multiple] {original\\recordings}; &
        \node (sm) [process] {editing/\\sub-mixing}; &
        \node (src) [database] {individual\\source signals}; &
        \node (ren) [process] {rendering\\\footnotesize(with panning)}; &
        \node (cons2) [terminal] {consumer}; \\
    };
\node (sd) [document, below=of src] {scene\\description};
\graph [use existing nodes] {
    or1 -> mp -> rec -> rep -> cons1;
    or2 -> sm -> src -> ren -> cons2;
    sd -> [out=east, in=190] ren;
};
\end{tikzpicture}
\caption{Panning during production of \emph{channel-based} content (above)
compared to panning during consumption of \emph{object-based} content (below)}
\label{fig:channel-vs-object}
\end{figure}

Switching from \emph{channel-based} to \emph{object-based}
can be seen as moving part of the mixing process
from the producer to the consumer
(see figure~\ref{fig:channel-vs-object}).
For example, panning can be used to create \emph{channel-based} mixes
-- which has been done since the late 1930s and is still done today at
a massive scale --
but it can also be used for the consumer-side rendering
of an \emph{object-based} recording.
These days -- as it has been the case since more than a century --
the movie industry is the biggest driver
for commercial spatial audio technology.
A hybrid of \emph{channel-based} and \emph{object-based}
movie sound tracks \parencite{robinson2012atmos}
have been widely used in cinemas in the last 10 years.
