\chapter{Movement and Time in Existing Formats}
\label{sec:existing-formats}

The end of the previous chapter has chronicled the emergence of
\emph{object-based} audio reproduction systems
and their need for scene descriptions.
A \emph{scene description}, in this context,
consists of a detailed description of \emph{sound objects},
including their source signals, their positions over time
and other information that is needed
to reproduce the desired auditory scene for one or more listeners
with any compatible reproduction system.
Over time, several scene description formats
have been proposed.
This chapter will present a few of those formats,
with special focus on how they handle the description of movement
(of sound sources and other scene objects) over time.
The list of formats shown here is by no means exhaustive,
and only formats are presented where public information is available,
which excludes some proprietary commercial formats.


\section{Recurring Concepts}

The following subsections will describe a few common concepts
that are relevant for multiple formats
and that hopefully make it easier
to discuss and categorize the individual formats
which will be presented after this section.

\subsection{Declarative vs.\ Procedural vs.\ Sampled Data}
\label{sec:declarative-procedural-sampled}

These terms are probably best explained by example.
Consider the sentence ``the sound of a bumblebee approaches from the far left,
circles the head of the listener two times and then stops abruptly.''
This sentence can be seen as a \emph{declarative} audio scene description.
It is a very high-level description and also very vague and inexact.
To make it actually usable in a real system,
more information probably has to be provided.
The trajectory of the bumblebee could be described more exactly by
providing a few coordinates and the times when those should be reached by the
bumblebee and maybe its velocity at those points.
The intermediate positions would be interpolated by the renderer
according to a pre-determined mathematical procedure.
This would still be considered \emph{declarative}.
Describing the sound itself in a \emph{declarative} way might be harder.
The bumblebee sound could be created by a generic synthesizer module,
and the scene description would contain
standardized parameters for this synthesizer
-- most likely changing over time --
like selection of sound generators and filter coefficients.

More realistically,
the sound would be provided by a natural recording of a bumblebee,
stored as a digital signal,
which is just a stream of \emph{sampled data}.
The sound could then be spatialized according to
a \emph{declarative} trajectory as described above.
However,
the trajectory could just as well be stored as a stream of coordinates,
regularly \emph{sampled} at certain intervals.
This is used in object-based cinema sound systems,
where the position data is typically sampled at \qty{30}{\hertz} or more
\parencite{riedmiller2015immersive}.
\emph{Sampled data} typically refers to temporal sampling,
but it could also be angular or other spatial sampling,
for example, when storing measured source directivity patterns
as part of a scene.

Some scene description formats
allow the usage of a scripting language to generate arbitrary movements
on the fly.
This is called \emph{procedural}, because the actual procedure
is stored as part of the scene description.
An important difference to the \emph{declarative} approach is
that the rendering engine has no knowledge about trajectories or sound
synthesis parameters or any such high-level constructs.
It would instead just render the source signal at
whatever position the script generates at any given time.
The signal itself could also be generated by a piece of software
that is distributed as part of the scene description,
for example as a software plugin.

Oftentimes the three concepts are mixed,
for instance in the X3D format
(see section~\ref{sec:X3D}):
\emph{declarative} trajectories can be defined by a small number of control points,
either with linear or smooth interpolation
(\code{Position\-Interpolator}, \code{Spline\-Position\-Interpolator}),
\emph{sampled data} is used for the source signals
(stored separately as conventional mono audio files), and
\emph{procedural} animations can be realized by means of \code{Script} nodes.


\subsection{Scene Graph}

A \emph{scene graph} is commonly used in 3D scene descriptions
as the top-level structural element.
It is a hierarchical, tree-like representation containing all scene components
as so-called \emph{nodes}.
Those scene components can be part of other nodes
which define their positions and orientations
with respect to a local coordinate system.
Those nodes can in turn be part of other nodes
with their own local coordinate systems and so on.
All nested containers contribute to the final position and orientation
of the scene components.
A change in the position, orientation or scaling of a node
affects all its child nodes.
For example, a car could be modelled as a node in the scene graph with a certain
position and orientation in the virtual world.
The car can then consist of multiple sub-nodes like tyres and seats and doors,
which are all defined in the local coordinate system of the car.
When the car node -- and therefore its local coordinate system --
is moved in the scene,
all its child nodes move together with it.
This is commonly used for visual scene descriptions,
but a scene graph could also be used for audio scenes.
In such a case, a car node could define a local coordinate system
containing multiple audio objects for the noise of the tyres
and another audio object with engine noise.
This makes for a very clear and well-structured spatial hierarchy,
which can be very useful to define a static 3D model
consisting of many elements which are again comprised of many sub-elements.

On the flip side, the temporal structure of a scene if often obscured.
For an example see the \code{ROUTE} node of VRML (section~\ref{sec:VRML})
which breaks the tree-like hierarchy
and connects events, timers and interpolators
with the scene properties that are supposed to be changed over time.
Routes, interpolators and other timing nodes are stored within the scene graph,
but they are not logically part of the hierarchy.
Moving a parent element has no effect on those timing nodes,
which means that they could basically be placed anywhere in the scene graph
without a change in behavior.

An alternative to the \emph{scene graph} concept is provided by the
SMIL format (see section~\ref{sec:SMIL}).
Instead of using spatial relationships for the main structural organization,
it focuses on temporal relationships
with a primary structure called \emph{time graph}.


\subsection{Metadata}

Many authors use the term \emph{metadata} to describe
spatial data -- trajectories, for example -- within a scene description,
maybe to distinguish it from \emph{audio data}.
The word \emph{metadata} means ``data about data''
and its use is inappropriate in this case.
Actual \emph{metadata} of an audio signal could be the date of recording,
the used microphone type or other equipment,
a description of the audible sound sources
and even their positions at the time of recording.
The intended positions and movements of sources in a scene description,
however,
are simply additional \emph{data} and not \emph{metadata},
just like the sound track is not \emph{metadata} of a movie.

An audio file can be swapped with another one
while the movement of the virtual sound source remains the same,
which confirms that the movement data is not \emph{metadata}.
A scene description can (and often does) contain actual metadata, though.
This could be -- among many other things --
the name of the author(s), date of creation
and a license governing the usage and re-distribution of the content.


\section{Virtual Reality Modeling Language (VRML)}
\label{sec:VRML}

The VRML is a storage format
mainly for 3D computer graphics,
developed in the early days of the \emph{world wide web}.
In addition to graphics,
it can also describe spatial audio sources,
including moving ones.
VRML version 2.0 -- also known as
VRML97\footnote{\url{%
https://www.web3d.org/documents/specifications/14772/V2.0/}}
--
became an ISO standard\footnote{\url{https://www.iso.org/standard/25508.html}}
in 1997.

The main structure of a VRML scene
is a scene graph,
which is built from (possibly nested) \code{Transform} nodes which define
nested local coordinate systems.
All visible geometric elements (defined by polygons),
as well as light sources, camera views
and also audio objects are added to this scene graph.
If the same data is needed repeatedly (\eg vertex coordinates),
it can be defined once with \code{DEF}
and used multiple times with \code{USE}.

The positions and orientations of all local coordinate systems of the scene
graph are specified statically.
Positions are given as triples of Cartesian coordinates
and orientations are given as
three numbers representing a normalized rotation vector
plus a fourth number representing the rotation angle in radians.
The movement of scene elements can be achieved in multiple ways.
\code{TimeSensor} nodes can be defined
to change certain properties of certain scene elements at given times
or at regular intervals.
Interactive changes can be triggered based on
the virtual position of the viewer
by means of \code{ProximitySensor} nodes.
Continuous position changes can be realized with
\code{PositionInterpolator} nodes, which interpolate linearly
between a given sequence of positions and their associated time values.
For continuous orientation changes, the
\code{OrientationInterpolator} node uses
\textbf{s}pherical \textbf{l}inear int\textbf{erp}olation (Slerp)\footnote{%
For an explanation of \emph{Slerp}
see section~\ref{rotation/slerp::doc} in the appendix.}
between a sequence of orientations and their associated time values.
The interpolation between two orientations always happens along the smallest
angle, which means that a single rotation step cannot be larger than 180
degrees.
If the angle is exactly 180 degrees, the result is explicitly undefined.
Proximity sensors can trigger time sensors,
which themselves can control the progression of interpolators.
All these nodes have to be connected by \code{ROUTE} commands.
The following (abridged) example
defines a simple scene showcasing position and orientation interpolation
as well as the \code{Sound} and \code{AudioClip} nodes,
which can be used to define sound sources
and their corresponding source signals:
\input{highlighted-code/vrml-example.wrl}

\noindent
In this example, the transform node called \code{Transform01}
is moved (\ie translated) by the interpolator named \code{PosInterp01}.
This interpolator is controlled
by the time sensor called \code{TimeSensor01}
which is in turn triggered by the proximity sensor named \code{ProxSensor01}.

As an alternative to interpolators,
\code{Script} nodes can be used to provide custom code
-- typically implemented in ECMAScript/JavaScript --
which allows generating arbitrary parameter progressions,
including the animation of position and orientation.
The inputs and outputs of \code{Script} nodes
are connected to other nodes by \code{ROUTE} commands.
The VRML standard also describes the so-called
External Authoring Interface (EAI),
which allows manipulating the scene graph during runtime
from external applications.


\section{Extensible 3D (X3D)}
\label{sec:X3D}

The successor of the VRML is X3D,
which is an
ISO standard\footnote{\url{https://www.iso.org/standard/60760.html}}
since 2004.
It is maintained by the
Web3D Consortium\footnote{\url{https://www.web3d.org/}}.
The previous version\footnote{\url{%
https://www.web3d.org/standards/version/V3.3}}
of the standard
was released in 2013
and the latest version\footnote{\url{https://www.web3d.org/x3d4}}
is just being released in 2023.

X3D builds on the same concept of a scene graph,
and most nodes from VRML are still available,
including
\code{Transform},
\code{Time\-Sensor},
\code{Proximity\-Sensor},
\code{Script},
\code{Position\-Interpolator} and \code{Ori\-en\-ta\-tion\-In\-ter\-po\-la\-tor}.
There is also a \code{ROUTE} element to connect events
and \code{DEF} and \code{USE} attributes to define and re-use elements.
There are also several new nodes, providing additional features
on top of VRML.
The
\code{Spline\-Position\-Interpolator} node
can be used to create trajectories along
Hermite splines, allowing to specify incoming and outgoing velocity vectors
at each control point
(see section~\ref{euclidean/hermite::doc} in the appendix).
If no velocity vectors are specified,
the tangents are automatically calculated to produce
\emph{Catmull--Rom splines}.
The X3D standard doesn't get the equations quite right,
for a correct derivation of the tangents for non-uniform Catmull--Rom splines
see section~\ref{euclidean/catmull-rom::doc} in the appendix.
The \code{Squad\-Orientation\-Interpolator}
can be used to animate orientations with
\textbf{s}pherical \textbf{quad}rangle interpolation (Squad)\footnote{%
For an explanation of \emph{Squad}
see section~\ref{rotation/squad::doc} in the appendix.}.

X3D has three syntaxes:
a new XML-based syntax,
the old VRML syntax and
a binary format for efficient storage and transmission.
Similar to the EAI in VRML,
X3D defines a Scene Access Interface (SAI)
for real-time interaction with external programs.
X3D has 4 baseline profiles: \emph{Interchange}, \emph{Interactive},
\emph{Immersive} and \emph{Full}.
The \code{Sound} and \code{AudioClip} elements
are part of the
\emph{Immersive} profile.

\input{highlighted-code/x3d-example.xml}

\noindent
Many more examples are available on the web.
It is even possible to embed
X3D scenes in HTML5 pages with
\emph{x3dom}\footnote{\url{https://www.x3dom.org/}}.


\section{MPEG-4 AudioBIFS}
\label{sec:MPEG-4}

MPEG-4 Audio is an ISO/IEC standard\footnote{\url{%
https://www.iso.org/standard/76383.html}}
since 1999.
A separate part of the standard\footnote{\url{%
https://www.iso.org/standard/63548.html}},
known as BInary Format for Scenes (BIFS),
which deals with the description of scenes,
contains a superset of the functionality provided by VRML.
It therefore inherits the \emph{scene graph} approach from VRML
including the
\code{Sound}, \code{AudioClip}, \code{Viewpoint},
\code{Position\-Interpolator} and \code{Orientation\-Interpolator} nodes.
The set of additional nodes for spatial audio
(for example \code{AudioSource}, \code{AudioBuffer}, \code{AudioMix}, \code{AudioFX})
is known under the name AudioBIFS
\parencite{scheirer1999audiobifs}.
However, no new nodes have been added
for describing the movement of sound sources.
The additional nodes from X3D
(\code{Spline\-Position\-Interpolator} and
\code{Squad\-Orientation\-Interpolator})
are not included.
Movement of sound sources can be achieved
by the same means as in VRML,
using event routing with \code{ROUTE}.
In addition,
``BIFS Animation'' commands can be included in the BIFS data stream,
and nodes in the scene graph can be added and replaced
with ``BIFS Update'' commands.

Scenes are stored and transmitted using a binary format,
but there is also an alternative textual representation called
eXtensible MPEG-4 Textual (XMT) \parencite{kim2000xmt}.
It comes in two flavors:
XMT-A has an XML-based syntax similar to X3D
(see section~\ref{sec:X3D}),
and
XMT-$\Omega$ has a syntax inspired by SMIL
(see section~\ref{sec:SMIL}).

A second version of MPEG-4
was published as an amendment to the standard in the year 2000.
The added audio nodes
\code{Acoustic\-Scene}, \code{Acoustic\-Material}, \code{Directive\-Sound}
and \code{Perceptual\-Parameters}
are also known as
Advanced AudioBIFS \parencite{vaananen2004advanced}.
These new nodes are used
for physical and perceptual modeling of acoustic environments,
but no features regarding sound source movement have been added.

A third version of AudioBIFS added the nodes
\code{Audio\-Channel\-Config},
\code{Transform3D\-Audio},
\code{WideSound},
\code{SurroundingSound} and
\code{AdvancedAudioBuffer}
and it changed the handling of \code{AudioFX} nodes
\parencite{schmidt2004mpeg4}.


\section{Audio3D}

Audio3D is an
XML-based audio scene description format
presented by \textcite{hoffmann2003audio3d}.
It uses a \emph{scene graph} to define the spatial arrangement.
The position of sound sources and of the listener can be animated
with the \code{<Animation>} element:
\input{highlighted-code/audio3d-position.xml}

\noindent
The ``direction'' -- presumably a direction vector -- can be animated as well:
\input{highlighted-code/audio3d-direction.xml}

\noindent
Not only spatial properties,
but also gain values can be animated with the same approach:
\input{highlighted-code/audio3d-gain.xml}

\noindent
The values between key frames are interpolated linearly.


\section{XML3DAUDIO}

Like the name suggests,
XML3DAUDIO is
an XML-based description format for
three-dimensional audio scenes.
It has been proposed and defined in
\parencite{potard2002schema,potard2003encoding,potard2004xml,potard2006object}.

Positions of sound sources are specified using
left-handed Cartesian coordinates (in meters).
Source orientations can be given with
\texttt{<Rotate>},
\texttt{<Tilt>} and
\texttt{<Tumble>} (in degrees).
A listener position and orientation can also be specified.
The listener's orientation can be given with
\texttt{<Azimuth>},
\texttt{<Elevation>} and
\texttt{<Roll>}.
Multiple listeners can be defined
and the point of view can be switched between them.

Dynamic movements of scene objects can be implemented with an
``orchestra and score'' approach
inspired by the venerable computer music software
Csound\footnote{\url{https://csound.com/}}.
The \emph{orchestra} part contains a list of objects
like listeners and sound sources together with some static attributes.
The \emph{score} part
contains \emph{lines of score},
each one incorporating one of a given set of \emph{opcodes}.
There are again two parts:
an \emph{initialization score} and a \emph{performance score}.
The former can for example be used for creating groups of sources.
The latter allows for changing scene parameters dynamically over time,
for example translating and rotating sound sources and listener objects.

\textcite[p.\,127]{potard2006object}
provides a single example scene in XML format:
\input{highlighted-code/xml3daudio-example.xml}

\noindent
The same example scene -- with minor modifications --
is also printed in \parencite[fig.\,5, p.\,4]{potard2004xml}.
Other than that, no further scenes seem to be publicly available.

The example shows how the ``move'' opcode can be used
to animate the translation of a sound source by providing target coordinates.
Presumably, linear interpolation is used.
According to the aforementioned literature,
it is also possible to dynamically control rotations
by a 3D rotation vector.
However, no example is provided.
Other scene parameters can be dynamically controlled with the
\texttt{ChangeParameter} opcode.

\textcite{potard2003encoding} mention that
``complex trajectories can be described by
the \texttt{TRAJ} opcode and a long list of coordinates.''
Sadly, no more information and no examples are provided.
In the other publications, this opcode is not even mentioned at all.


\section{Audio Definition Model (ADM)}
\label{sec:adm}

The ADM is an audio scene description format
defined by ITU-R in Recommendation
BS.2076\footnote{\url{https://www.itu.int/rec/R-REC-BS.2076/}}.
Its latest edition BS.2076-2 was published in 2019.

The ADM is an XML-based format
that's typically embedded
in the \texttt{<axml>} chunk of Broadcast Wave Format (BWF)
files, as specified by ITU-R in Recommendation
BS.2088\footnote{\url{https://www.itu.int/rec/R-REC-BS.2088/}}.
The \texttt{<chna>} chunk is used
to associate the track ID used in the XML description
with the appropriate audio channels from the BWF file.

Positions of sound sources are by default stored
using a spherical coordinate system with
\texttt{azimuth}
and
\texttt{elevation}
angles in degrees and
\texttt{distance}
in relative units.
Alternatively, Cartesian coordinates can be used with
\texttt{X} (to the right),
\texttt{Y} (forward) and
\texttt{Z} (up) in relative units.
Relative units are mapped to physical units with the
\texttt{absoluteDistance} value (in meters).
All coordinates are understood as relative to a fixed listener position.
If the listener's head rotation is tracked during playback,
the \texttt{headLocked} flag can be used to interpret the given
coordinates as relative to the head rotation.
Other than that, there is no way to specify any rotations.

Positions (and other dynamic scene parameters)
are assigned to sound sources using \texttt{<audioBlockFormat>} elements.
Each of those elements can only contain one position value.
To define moving sound sources, multiple such elements have to be used.
Each block has to have a start time (\texttt{rtime})
relative to the start time of the parent element
and a \texttt{duration} value in seconds.

Example (simplified excerpt) from annex 2,
section 2.3 of the ADM specification:
\input{highlighted-code/adm-excerpt.xml}

\noindent
By default,
the values are interpolated between blocks.
This means that at the beginning of a block
the previous block's value is still used
and the value specified for the current block
is only reached at the end of the block.
This also means that the very first block
has an undefined value.
In this case, the ADM specification recommends
setting the \texttt{jumpPosition} flag,
which applies the given value as a constant to the whole block.
Interpolations can also be limited to a shorter time than the block duration
with the \texttt{interpolationLength} value (in seconds),
but this is discouraged by the specification.
The ADM specification doesn't mention any further details
about the exact type of interpolation to be used,
but the illustrated examples hint at linear interpolation.
Experts at ITU-R are currently working on a revision
that will hopefully clarify the situation.

Apart from the positions of sound sources,
their physical size can be specified with
\texttt{width},
\texttt{depth} and
\texttt{height} (in relative units).

Source signals can be boosted or attenuated by means of the
\texttt{gain} value, which is interpreted as a linear value by default.
The \texttt{gainUnit} option can be used to switch to decibels (dB).
Gain values are interpolated just like positions,
but again, the exact shape of the interpolation is not specified.


\section{Spatial Sound Description Interchange Format (SpatDIF)}
\label{sec:SpatDIF}

SpatDIF was first proposed in
\parencite{peters2007spatdif},
where it is described as a stream of
Open Sound Control (OSC)
messages which can optionally be stored in
Sound Description Interchange Format (SDIF)
files.
Sound source positions are encoded in a
listener-relative normalized coordinate system.
They can be specified either in cartesian (x, y, z)
or spherical (azimuth, elevation, distance) coordinates:
\input{highlighted-code/spatdif-positions.osc}

\noindent
In \parencite{peters2008spatdif}, the coordinate system(s) can be chosen nearly
arbitrarily:
\input{highlighted-code/spatdif-coordinate-systems.osc}

\noindent
As addition to a stream of tightly sampled values
without any high-level structure,
\parencite{peters2013spatdif}
introduces a so-called
\emph{authoring layer}
which theoretically allows defining complex movements more compactly.
No concrete examples are given,
but some ideas for a trajectory extension can be found on
the project's (archived) Wiki page\footnote{\url{%
https://web.archive.org/web/20220629192742/http://redmine.spatdif.org/projects/spatdif/wiki/Trajectory_Extension}}.

The paper stresses that
``SpatDIF is a syntax rather than a programming interface or file format''
and the examples for file storage
are expanded to use XML and YAML
(in addition to the aforementioned OSC and SDIF).
An abridged version of one of the
example files\footnote{\url{%
https://web.archive.org/web/20220819072708/http://spatdif.org/examples.html}}
is provided here in OSC format:
\input{highlighted-code/spatdif-example.osc}

\noindent
The individual OSC messages do not carry timestamps,
but \texttt{time} messages can be used to provide
timing information for the immediately following messages.
The same example is also provided in XML format:
\input{highlighted-code/spatdif-example.xml}

\noindent
The initial SpatDIF tools were only able to run on proprietary software.
Later, an fully open source library
implementation\footnote{\url{https://github.com/SpatDIF/SpatDIFLib}}
was presented in \parencite{miyama2013spatdif}.

Version 0.4 of SpatDIF is presented in
\parencite{schacher2016spatdif},
which promises
``the ability to define
and store continuous trajectories on the authoring layer in
a human-readable way.''
Cubic Bézier curves (see section~\ref{euclidean/bezier::doc} in the appendix)
can be used to define trajectories.
By default, movement along those trajectories happens with a constant speed,
but \emph{easing curves} can be specified --
either selected from a number of easing functions
or defined by yet another cubic Bézier curve.
In the latter case, a nearly arbitrary mapping between time and position along
the curve can be provided, which allows moving
sound sources forwards and backwards along a trajectory.
Due to backwards compatibility concerns,
SpatDIF v0.4 doesn't allow transmitting or storing
trajectories on their own.
A sampled (discretized) version of each trajectory has to be stored as well.
According to \textcite{schacher2016spatdif},
``\dots authoring is expected to
be done with software tools that provide graphical user interfaces;
hence there should be little or no need to interact
directly with cubic-bezier parameter values.''
Unfortunately, no information about version 0.4 could be found on the
(now defunct)
SpatDIF website
and no example files seem to be available.


\section{Spat-SDIF}

The Sound Description Interchange Format (SDIF)
does not natively support spatialization,
but as suggested in the previous section,
it can be extended for storing
source positions and similar data.
One way of doing this is by using
Spat-SDIF as described in
\parencite{bresson2011spatialization}.
One or more control values can be stored in an SDIF frame,
and each of these frames contains its own timestamp.
The Spat-SDIF player
application\footnote{\url{https://github.com/j-bresson/Spat-SDIF-Player}}
can generate real-time SpatDIF-compatible OSC
messages.


\section{Toolbox for Acoustic Scene Creation And Rendering (TASCAR)}

TASCAR is a software for
creation and rendering of dynamic virtual acoustic environments,
developed for application in hearing aid research and audiology
\parencite{grimm2019tascar}.
It comes with its own XML-based file format.
This is a snippet from the example file
\code{example\_vertices.tsc},
as shown in the user manual\footnote{\url{https://tascar.org/manual.pdf}}:
\input{highlighted-code/tascar-example.xml}

\noindent
Sound sources can have trajectories,
which can be defined by specifying coordinate triples,
prepended with the time (in seconds) of their occurrence.
Position values between those times are linearly interpolated.
An \emph{interpolation mode} can be chosen between
\emph{cartesian} (which is the default)
and \emph{spherical}.
Latter mode interpolates in arcs around the origin,
which is probably only useful if the receiver is positioned at the origin,
which, admittedly, is a common case.

Sound sources can also have an orientation,
which -- unlike in most computer graphics applications --
is applied to the source object after the position.
The interpolation of orientations
is not explicitly mentioned in the user manual,
but according to the current implementation,
the three Euler angles seem to be interpolated separately,
which works for rotations around one of the coordinate axes,
but it hopelessly breaks in the general case,
see section~\ref{rotation/naive-euler-angles-interpolation::doc}
of the appendix.

% relevant code comment for broken orientation interpolation:
% https://github.com/HoerTech-gGmbH/tascar/blob/3e99b00bd0cdd5832f5a5e229303f01c0814ca0b/libtascar/include/coordinates.h#L501



\section{Synchronized Multimedia Integration Language (SMIL)}
\label{sec:SMIL}

The SMIL --
which is supposed to be pronounced like ``smile'' --
is a format for
temporal control and synchronization of audio, video, images and text elements
and their arrangement on a 2D screen.
It is a recommendation\footnote{\url{https://www.w3.org/TR/SMIL/}}
of the World Wide Web Consortium
since 1998; the current version (SMIL 3.0) was released in 2008.
In contrast to the formats mentioned in previous sections,
SMIL was never intended for describing 3D scenes,
but extending it to three dimensions has been suggested
\parencite{goose2002streaming,pihkala2003extending}.

However,
the interesting part of this format are not its spatial aspects
nor its rather limited audio capabilities
but rather its handling of the
timing\footnote{\url{https://www.w3.org/TR/SMIL/smil-timing.html}}
of content elements.
Most notably, it uses so-called \emph{time containers}
as a top-level structure.
All elements in a \code{<par>} container
are played back in \emph{parallel} and
all elements in a \code{<seq>} container
are played back \emph{sequentially}.
These containers can be arbitrarily nested,
building up a single hierarchical structure called \emph{time graph}.
Inside those time containers, media files
are linked to the SMIL file with
\code{<img>},
\code{<audio>},
\code{<text>} and similar elements.
The following example shows how
the time containers can be used to define a slide show.
The background image and music are played/shown in parallel to slides,
which are themselves shown in sequence.

\input{highlighted-code/smil-example.xml}

\noindent
In this case, there is no need to specify any start times
because the start times of the slides are determined
by the duration of the preceding slides.
If desired, however, elements can have a start and end time
relative to their parent or sibling elements.
Those times can also be specified
relative to the start or end of non-adjacent elements
in the time graph and they can be triggered by user actions,
for example mouse clicks.
Graphical elements can be animated along
linear or smooth 2D-paths with the \code{animateMotion} element.


\section{Bottom Line}

None of the presented 3D audio scene description formats
explicitly targets scene authoring.
All of the text-based formats can of course be created in a plain text editor,
but their syntaxes are not optimized for manual creation.
The declarative definition of source movements
is often limited to linear interpolation.
One counter-example that allows smooth interpolation
is X3D (see section~\ref{sec:X3D}).
In X3D, time instances can be specified for each control point,
which affects the velocity along the trajectory.
However, the resulting speed between control points might be unexpected.
It is not possible to control
the shape and the speed of a trajectory separately and
it is not straightforward to define a smooth trajectory with constant speed.
Interpolation between orientations is in most cases not supported,
except for Slerp in VRML (section~\ref{sec:VRML})
and Squad in X3D.
In the latter case, the standard text is quite vague
and it is doubtful whether different implementations agree in their behavior
-- if this feature is implemented at all.
The next chapter will present a new format that tries to overcome
these shortcomings.
